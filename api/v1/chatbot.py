from fastapi import APIRouter
from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer

# Create a new router
router = APIRouter()

# Load the model and tokenizer
model_name = "facebook/blenderbot-400M-distill"
model = BlenderbotForConditionalGeneration.from_pretrained(model_name)
tokenizer = BlenderbotTokenizer.from_pretrained(model_name)

# Initialize conversation history
conversation_history = []

def generate_response(user_input):
    """
    Generate response for the given user input using the loaded model and tokenizer.

    Args:
    - user_input (str): The input provided by the user.

    Returns:
    - str: The response generated by the model.
    """
    # Append user input to conversation history
    conversation_history.append({"speaker": "user", "text": user_input})
    
    # Concatenate user and bot messages into a single string
    conversation = ""
    for item in conversation_history:
        conversation += item["speaker"] + ": " + item["text"] + "\n"
    
    # Generate response
    input_ids = tokenizer.encode(conversation, return_tensors="pt")
    response_ids = model.generate(input_ids, max_length=100, num_return_sequences=1, temperature=0.9)
    response = tokenizer.decode(response_ids[0], skip_special_tokens=True)
    
    # Append model response to conversation history
    conversation_history.append({"speaker": "bot", "text": response})

    return response

# Define the route handler for the chatbot endpoint
@router.get("/chatbot")
async def chatbot_response(question: str):
    response = generate_response(question)
    return {"result": response}




# from fastapi import APIRouter
# from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer

# # Create a new router
# router = APIRouter()

# # Load the model and tokenizer
# model_name = "facebook/blenderbot-400M-distill"
# model = BlenderbotForConditionalGeneration.from_pretrained(model_name)
# tokenizer = BlenderbotTokenizer.from_pretrained(model_name)

# # Initialize conversation history
# conversation_history = []

# def generate_response(user_input):
#     """
#     Generate response for the given user input using the loaded model and tokenizer.

#     Args:
#     - user_input (str): The input provided by the user.

#     Returns:
#     - str: The response generated by the model.
#     """
#     # Append user input to conversation history
#     conversation_history.append({"speaker": "user", "text": user_input})
    
#     # Concatenate user and bot messages into a single string
#     conversation = ""
#     for item in conversation_history:
#         conversation += item["speaker"] + ": " + item["text"] + "\n"
    
#     # Generate response
#     input_ids = tokenizer.encode(conversation, return_tensors="pt")
#     response_ids = model.generate(input_ids, max_length=1000, num_return_sequences=1)
#     response = tokenizer.decode(response_ids[0], skip_special_tokens=True)
    
#     # Append model response to conversation history
#     conversation_history.append({"speaker": "bot", "text": response})

#     return response

# # Define the route handler for the chatbot endpoint
# @router.get("/chatbot")
# async def chatbot_response(title: str):
#     response = generate_response(title)
#     return {"result": conversation_history}
